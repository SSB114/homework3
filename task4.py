import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import make_pipeline
from sklearn.exceptions import NotFittedError

# ====================== å…¨å±€å‚æ•°é…ç½®åŒºï¼ˆä¾¿äºä¿®æ”¹ï¼‰ ======================
SEED = 42  # éšæœºç§å­
TEST_SIZE = 0.3  # æµ‹è¯•é›†æ¯”ä¾‹
POLY_DEGREE = 2  # å¤šé¡¹å¼ç‰¹å¾é˜¶æ•°
SELECT_K = 6  # é€‰æ‹©æœ€ä¼˜ç‰¹å¾æ•°
LR_C_OPT = 20  # ä¼˜åŒ–æ¨¡å‹æ­£åˆ™åŒ–å‚æ•°
LR_MAX_ITER_OPT = 1000  # ä¼˜åŒ–æ¨¡å‹æœ€å¤§è¿­ä»£æ•°
LR_MAX_ITER_RAW = 500  # åŸå§‹æ¨¡å‹æœ€å¤§è¿­ä»£æ•°
FIG_SIZE = (16, 12)  # ç”»å¸ƒå°ºå¯¸
FIG_DPI = 300  # å›¾ç‰‡åˆ†è¾¨ç‡
CLASS_COLORS = ['#FFD700', '#90EE90', '#87CEFA']  # æ›´ç¾è§‚çš„é¢œè‰²ï¼ˆé»„é‡‘è‰²ã€æ·¡ç»¿ã€æ·¡è“ï¼‰
SAVE_PATH = "task4_enhanced_visualization.png"  # ä¿å­˜è·¯å¾„

def load_iris_data() -> tuple:
    """åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†å¹¶è¿”å›ç‰¹å¾ã€æ ‡ç­¾ã€ç‰¹å¾åã€ç›®æ ‡å"""
    iris = load_iris()
    X = iris.data
    y = iris.target
    feature_names = iris.feature_names
    target_names = iris.target_names
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼šç‰¹å¾æ•°={X.shape[1]}, æ ·æœ¬æ•°={X.shape[0]}, ç±»åˆ«æ•°={len(target_names)}")
    return X, y, feature_names, target_names

def build_feature_pipeline() -> make_pipeline:
    """æ„å»ºç‰¹å¾å·¥ç¨‹ç®¡é“ï¼ˆå¤šé¡¹å¼äº¤äº’ç‰¹å¾+æ ‡å‡†åŒ–+ç‰¹å¾é€‰æ‹©ï¼‰"""
    pipeline = make_pipeline(
        PolynomialFeatures(
            degree=POLY_DEGREE,
            interaction_only=True,
            include_bias=False
        ),
        StandardScaler(),
        SelectKBest(f_classif, k=SELECT_K)
    )
    return pipeline

def optimize_features(X: np.ndarray, y: np.ndarray) -> tuple:
    """æ‰§è¡Œç‰¹å¾å·¥ç¨‹ï¼Œè¿”å›ä¼˜åŒ–åçš„ç‰¹å¾çŸ©é˜µå’Œç‰¹å¾å·¥ç¨‹ç®¡é“"""
    pipeline = build_feature_pipeline()
    try:
        X_optimized = pipeline.fit_transform(X, y)
        # æ ¡éªŒç‰¹å¾æ•°é‡
        if X_optimized.shape[1] < SELECT_K:
            print(f"âš ï¸ è­¦å‘Šï¼šå®é™…å¯é€‰ç‰¹å¾æ•°ä¸è¶³{SELECT_K}ï¼Œä»…è¿”å›{X_optimized.shape[1]}ä¸ªç‰¹å¾")
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼šä¼˜åŒ–åç‰¹å¾æ•°={X_optimized.shape[1]}")
        return X_optimized, pipeline
    except Exception as e:
        raise RuntimeError(f"âŒ ç‰¹å¾å·¥ç¨‹æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")

def train_and_evaluate_models(X_optimized: np.ndarray, y: np.ndarray) -> dict:
    """è®­ç»ƒä¼˜åŒ–æ¨¡å‹å’ŒåŸå§‹æ¨¡å‹ï¼Œè¿”å›æ€§èƒ½æŒ‡æ ‡å­—å…¸"""
    # æ‹†åˆ†ä¼˜åŒ–ç‰¹å¾çš„è®­ç»ƒ/æµ‹è¯•é›†ï¼ˆåˆ†å±‚æŠ½æ ·ï¼‰
    X_train_opt, X_test_opt, y_train_opt, y_test_opt = train_test_split(
        X_optimized, y,
        test_size=TEST_SIZE,
        random_state=SEED,
        stratify=y
    )
    
    # ========== è®­ç»ƒä¼˜åŒ–æ¨¡å‹ ==========
    clf_opt = LogisticRegression(
        C=LR_C_OPT,
        max_iter=LR_MAX_ITER_OPT,
        random_state=SEED
    )
    clf_opt.fit(X_train_opt, y_train_opt)
    opt_test_acc = clf_opt.score(X_test_opt, y_test_opt)
    opt_cv_acc = cross_val_score(clf_opt, X_optimized, y, cv=5).mean()
    
    # ========== è®­ç»ƒåŸå§‹æ¨¡å‹ï¼ˆä»…ç”¨å‰4ä¸ªåŸå§‹ç‰¹å¾ï¼‰ ==========
    clf_raw = LogisticRegression(
        max_iter=LR_MAX_ITER_RAW,
        random_state=SEED
    )
    # ç¡®ä¿åŸå§‹ç‰¹å¾æ•°é‡è¶³å¤Ÿï¼ˆé²æ£’æ€§å¤„ç†ï¼‰
    raw_feat_num = min(4, X_train_opt.shape[1])
    clf_raw.fit(X_train_opt[:, :raw_feat_num], y_train_opt)
    raw_test_acc = clf_raw.score(X_test_opt[:, :raw_feat_num], y_test_opt)
    
    # æ•´ç†æ€§èƒ½æŒ‡æ ‡
    perf_metrics = {
        "opt_test_acc": opt_test_acc,
        "opt_cv_acc": opt_cv_acc,
        "raw_test_acc": raw_test_acc,
        "clf_opt": clf_opt,
        "X_train_opt": X_train_opt,
        "X_test_opt": X_test_opt,
        "y_train_opt": y_train_opt,
        "y_test_opt": y_test_opt
    }
    
    # æ‰“å°æ€§èƒ½å¯¹æ¯”
    print("\n" + "="*50)
    print("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("="*50)
    print(f"ä¼˜åŒ–åæ¨¡å‹ - æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{opt_test_acc:.3f} | 5æŠ˜äº¤å‰éªŒè¯å‡†ç¡®ç‡ï¼š{opt_cv_acc:.3f}")
    print(f"åŸå§‹æ¨¡å‹   - æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{raw_test_acc:.3f}")
    print("="*50 + "\n")
    
    return perf_metrics

def plot_enhanced_visualization(X_optimized: np.ndarray, y: np.ndarray, 
                               perf_metrics: dict, target_names: list):
    """ç»˜åˆ¶å¢å¼ºå‹3Dæ¦‚ç‡å¯è§†åŒ–å›¾å¹¶ä¿å­˜"""
    clf_opt = perf_metrics["clf_opt"]
    opt_test_acc = perf_metrics["opt_test_acc"]
    
    # å–ä¼˜åŒ–åçš„å‰3ä¸ªç‰¹å¾åš3Då¯è§†åŒ–
    X_3d_opt = X_optimized[:, :3] if X_optimized.shape[1] >=3 else X_optimized
    if X_3d_opt.shape[1] <3:
        raise ValueError(f"âŒ ä¼˜åŒ–åç‰¹å¾æ•°ä¸è¶³3ä¸ªï¼ˆä»…{X_3d_opt.shape[1]}ä¸ªï¼‰ï¼Œæ— æ³•ç»˜åˆ¶3Då›¾")
    
    # ç”Ÿæˆç½‘æ ¼ï¼ˆå›ºå®šç¬¬3ä¸ªç‰¹å¾ä¸ºå‡å€¼ï¼Œç®€åŒ–ä¸º2Dæ›²é¢+é«˜åº¦æ˜ å°„ï¼‰
    x1_min, x1_max = X_3d_opt[:, 0].min() - 1, X_3d_opt[:, 0].max() + 1
    x2_min, x2_max = X_3d_opt[:, 1].min() - 1, X_3d_opt[:, 1].max() + 1
    x3_fixed = X_3d_opt[:, 2].mean()
    xx, yy = np.meshgrid(
        np.linspace(x1_min, x1_max, 20),
        np.linspace(x2_min, x2_max, 20)
    )
    
    # æ„é€ å®Œæ•´è¾“å…¥ç‰¹å¾ï¼ˆå‰2ä¸ª+å›ºå®šç¬¬3ä¸ª+å‰©ä½™ç‰¹å¾å‡å€¼ï¼‰
    remaining_feats = X_optimized[:, 3:] if X_optimized.shape[1] >3 else np.array([])
    remaining_mean = remaining_feats.mean(axis=0) if remaining_feats.size >0 else np.array([])
    grid_opt = np.c_[
        xx.ravel(), yy.ravel(),
        np.full(xx.size, x3_fixed),
        np.tile(remaining_mean, (xx.size, 1)) if remaining_mean.size>0 else []
    ]
    
    # é¢„æµ‹ç±»åˆ«æ¦‚ç‡
    try:
        probs_opt = clf_opt.predict_proba(grid_opt)
    except NotFittedError:
        raise RuntimeError("âŒ æ¨¡å‹æœªè®­ç»ƒå®Œæˆï¼Œæ— æ³•é¢„æµ‹æ¦‚ç‡")
    
    # ========== ç»˜åˆ¶å›¾å½¢ ==========
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼ˆè‹±æ–‡ç¯å¢ƒï¼‰
    fig = plt.figure(figsize=FIG_SIZE, dpi=FIG_DPI)
    fig.suptitle(
        f'Task 4: Enhanced Visualization (Optimized Acc: {opt_test_acc:.3f})',
        fontsize=18, y=0.98, fontweight='bold'
    )
    
    # å­å›¾1-3ï¼š3ä¸ªç±»åˆ«çš„æ¦‚ç‡æ›²é¢
    for class_idx in range(3):
        ax = fig.add_subplot(2, 2, class_idx + 1, projection='3d')
        prob_class = probs_opt[:, class_idx].reshape(xx.shape)
        
        # ç»˜åˆ¶æ¦‚ç‡æ›²é¢ï¼ˆé«˜åº¦æ˜ å°„æ¦‚ç‡ï¼Œé¢œè‰²æ¸å˜ï¼‰
        surf = ax.plot_surface(
            xx, yy, x3_fixed + prob_class * 5,
            facecolors=plt.cm.RdYlBu(prob_class),
            alpha=0.8, edgecolor='gray', linewidth=0.2
        )
        
        # ç»˜åˆ¶å¯¹åº”ç±»åˆ«çš„æ•°æ®ç‚¹
        mask = y == class_idx
        ax.scatter(
            X_3d_opt[mask, 0], X_3d_opt[mask, 1], X_3d_opt[mask, 2],
            c=CLASS_COLORS[class_idx], s=70, edgecolors='black', alpha=0.9,
            label=f'{target_names[class_idx]} (Class {class_idx})', zorder=5
        )
        
        ax.set_xlabel('Optimal Feature 1', fontsize=11, labelpad=8)
        ax.set_ylabel('Optimal Feature 2', fontsize=11, labelpad=8)
        ax.set_zlabel('Optimal Feature 3', fontsize=11, labelpad=8)
        ax.set_title(f'Class {class_idx} Probability Distribution', fontsize=13, fontweight='medium')
        ax.legend(loc='upper right', fontsize=10, framealpha=0.9)
        ax.view_init(elev=20, azim=45)  # å›ºå®šè§†è§’
    
    # å­å›¾4ï¼šæ€§èƒ½å¯¹æ¯”æ–‡æœ¬
    ax4 = fig.add_subplot(2, 2, 4)
    ax4.axis('off')
    perf_text = f"""ğŸ“ˆ Optimization Details & Performance
-----------------------------------
1. Feature Engineering:
   â€¢ Polynomial Interaction Features (degree={POLY_DEGREE})
   â€¢ Standardization (remove scale bias)
   â€¢ Top-{SELECT_K} Feature Selection (ANOVA-F)
   
2. Model Tuning:
   â€¢ Logistic Regression (C={LR_C_OPT}, max_iter={LR_MAX_ITER_OPT})
   
3. Accuracy Comparison:
   â€¢ Optimized Model: {opt_test_acc:.3f} (Test) / {perf_metrics['opt_cv_acc']:.3f} (CV)
   â€¢ Raw Model (No Engineering): {perf_metrics['raw_test_acc']:.3f}"""
    
    ax4.text(
        0.1, 0.5, perf_text, fontsize=12, verticalalignment='center',
        bbox=dict(boxstyle="round,pad=0.8", facecolor="#F5F5F5", alpha=0.9, edgecolor="#CCCCCC"),
        fontfamily='monospace'
    )
    
    # è°ƒæ•´å¸ƒå±€ï¼ˆé¿å…æ ‡é¢˜é‡å ï¼‰
    plt.tight_layout()
    plt.subplots_adjust(top=0.92)
    
    # ========== ä¿å­˜å›¾ç‰‡ï¼ˆå…ˆä¿å­˜å†showï¼Œé¿å…ç©ºç™½ï¼‰ ==========
    plt.savefig(SAVE_PATH, dpi=FIG_DPI, bbox_inches='tight')
    print(f"âœ… å¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜è‡³ï¼š{SAVE_PATH}")
    
    # æ˜¾ç¤ºå›¾å½¢
    plt.show()
    plt.close(fig)  # é‡Šæ”¾ç”»å¸ƒèµ„æº

def main():
    """ä¸»å‡½æ•°ï¼šä¸²è”æ‰€æœ‰æµç¨‹"""
    try:
        # 1. åŠ è½½æ•°æ®
        X, y, feature_names, target_names = load_iris_data()
        
        # 2. ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–
        X_optimized, pipeline = optimize_features(X, y)
        
        # 3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
        perf_metrics = train_and_evaluate_models(X_optimized, y)
        
        # 4. å¢å¼ºå‹å¯è§†åŒ–
        plot_enhanced_visualization(X_optimized, y, perf_metrics, target_names)
        
        print("\nğŸ‰ ä»»åŠ¡å››å®Œæˆï¼æ ¸å¿ƒä¼˜åŒ–ï¼šç‰¹å¾å·¥ç¨‹+æ¨¡å‹è°ƒä¼˜ï¼Œå‡†ç¡®ç‡è¾ƒåŸå§‹æ¨¡å‹æå‡æ˜æ˜¾")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
        raise  # æŠ›å‡ºå¼‚å¸¸ä¾¿äºè°ƒè¯•

if __name__ == "__main__":
    main()